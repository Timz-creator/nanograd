# nanograd
# Nanograd: Autograd Engine from First Principles

A minimal autograd engine built to understand the mechanics behind PyTorch's automatic differentiation. Implements backpropagation on a dynamically built computational graph.

## ðŸŽ¯ Project Goals

- Understand automatic differentiation deeply
- Implement backpropagation from scratch
- Build computational graphs
- Train neural networks without frameworks


## ðŸ“š Resources

- [Andrej Karpathy's micrograd](https://github.com/karpathy/micrograd)


Building this as preparation for:
- AI Safety research understanding
- Deep learning fundamentals
- Robotics

Previous work: [Efficient attention mechanisms for embedded vision systems](https://github.com/timarleyfoster)

---

**Status:** ðŸš§ In Progress  

**Author:** Timarley Foster
